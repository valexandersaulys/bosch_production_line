9/7/2016
------------
First off, the datasets are ***very large***. In fact, they cannot fit into
memory so a generator will need to be built to properly process them.

Second, there are __many__ features to consider:
::train_categorical.columns >>> length=2141  # all numerical
::train_numeric.columns >>> length=970       # pd.get_dummies(...)
::train_date.columns >>> length=1157         # all numerical
Altogether --> 4268 features(!)

Some are 'nan' for long periods of time (i.e. categories only appear in sparse
areas). This happens to all three, categorical seems to stick out as
particularly sparse though.

Lastly, none of the cateogires have english names, they are all coded out.

Eyeballing 'train_date' suggests that its categorical, so I'll treat it as such


Questions to Ask:
  * Are there any scikit learn methods that will accept 'nan' values gracefully?

Steps to build:
  * Impute missing data on _numerical, consider 'nan' its own categorical for
    _categorical and _date. Output these into new CSVs 
  * Join all three CSVs together into one big CSV (gulp)
  * A train/valid/test splitter that will split up all the data (gulp), which
    should ideally be done on my local machine if it can handle it. Save these
    to separate csv files.
  * Complete PCA/LDA/QDA/SVD and general new features, save these to separate
    CSV files as well (maybe __with__ the original data too?)
  * Use feature selectors like looking for high variance features and using
    overfitted randomForest (multiple times to check) to see which features
    are most necesary.
  * Throw this all into xgboost for a final model
  * Double check on the validation set for hyperparameter selection
  * Use the test set to do final checks

----------------------------------------------------------------------------(80)
9 / 8 / 2016
============

train_numeric has 1,183,748 lines in it

It takes a fairly long time to do the imputing. This can be seen in 'find_dict_value.py',
which I'll funnel in piece by piece into the main python.

I added a generator that I scalped from a previous contest as well. 

----------------------------------------------------------------------------(80)
9 / 10 / 2016
============

Might it make more sense to insert the mean and median values at compute time as
opposed to building whole new CSV files?

I decided to do just that as I had troubles with the imputed values into their
own CSVs.

I'm thinking that all joins for the data should be done on 'readin_data.py'

Looking into breaking up the category columns in different columns; there's an
issue though as pd.get_dummies(...) and np.unique(...) do not return correct
values. Trying to figre out a way around.

I'm trying to run the following as I type, but its taking a while. Using a list
to brute force search for unique values:
>>> unique = [used.append(x) for x in df.values.tolist() if x not in used]
http://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python
Getting weird results, it ran for a long time so I gave up (maybe ~90 minutes)
