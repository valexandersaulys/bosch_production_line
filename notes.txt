URL: https://www.kaggle.com/c/bosch-production-line-performance
Due: November 11th, 2016
----------------------------------------------------------------------------(80)
9/7/2016
=========
First off, the datasets are ***very large***. In fact, they cannot fit into
memory so a generator will need to be built to properly process them.

Second, there are __many__ features to consider:
::train_categorical.columns >>> length=2141  # all numerical
::train_numeric.columns >>> length=970       # pd.get_dummies(...)
::train_date.columns >>> length=1157         # all numerical
Altogether --> 4268 features(!)

Some are 'nan' for long periods of time (i.e. categories only appear in sparse
areas). This happens to all three, categorical seems to stick out as
particularly sparse though.

Lastly, none of the cateogires have english names, they are all coded out.

Eyeballing 'train_date' suggests that its categorical, so I'll treat it as such


Questions to Ask:
  * Are there any scikit learn methods that will accept 'nan' values gracefully?

Steps to build:
  * Impute missing data on _numerical, consider 'nan' its own categorical for
    _categorical and _date. Output these into new CSVs 
  * Join all three CSVs together into one big CSV (gulp)
  * A train/valid/test splitter that will split up all the data (gulp), which
    should ideally be done on my local machine if it can handle it. Save these
    to separate csv files.
  * Complete PCA/LDA/QDA/SVD and general new features, save these to separate
    CSV files as well (maybe __with__ the original data too?)
  * Use feature selectors like looking for high variance features and using
    overfitted randomForest (multiple times to check) to see which features
    are most necesary.
  * Throw this all into xgboost for a final model
  * Double check on the validation set for hyperparameter selection
  * Use the test set to do final checks


----------------------------------------------------------------------------(80)
9 / 8 / 2016
============
train_numeric has 1,183,748 lines in it

It takes a fairly long time to do the imputing. This can be seen in 'find_dict_value.py',
which I'll funnel in piece by piece into the main python.

I added a generator that I scalped from a previous contest as well. 

----------------------------------------------------------------------------(80)
9 / 10 / 2016
============
Might it make more sense to insert the mean and median values at compute time as
opposed to building whole new CSV files?

I decided to do just that as I had troubles with the imputed values into their
own CSVs.

I'm thinking that all joins for the data should be done on 'readin_data.py'

Looking into breaking up the category columns in different columns; there's an
issue though as pd.get_dummies(...) and np.unique(...) do not return correct
values. Trying to figre out a way around.

I'm trying to run the following as I type, but its taking a while. Using a list
to brute force search for unique values:
>>> unique = [used.append(x) for x in df.values.tolist() if x not in used]
http://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python
Getting weird results, it ran for a long time so I gave up (maybe ~90 minutes)

----------------------------------------------------------------------------(80)
9 / 11 / 2016
==============
So I figured out that the pd.get_dummies(...) was missing a parameter to account
for 'NaN' values, namely pd.get_dummies(df,dummy_na=True).

Figured out how to write categorical data via multicore processing because its
going to take a long time (~30 hours) otherwise.


----------------------------------------------------------------------------(80)
9 / 12 / 2016
=============
Added a time elapsed bit to the 'ohe_categorical.py' and its taking forever to
run, so I'll queue it up to run tomorrow morning while I'm at work.


----------------------------------------------------------------------------(80)
9 / 24 / 2016
=============
So I did run it on the 13th and now I'm getting back to this contest after a
few weeks. The categorical data when one hot encoded has 8538 features.
  -> 'test_categorical.csv' also needs to be run this way. Likely will be done on
     Monday.
Now I'm going to stitch it all together with 'assemble_data.py' to allow for
better splitting. Currently sitting on ~120gb of CSV data (oof).

I'm still getting 'NaN' values for some date values (like in 'L3_S51_D4255').
Given there are too many values, I'll have to impute values instead. Rewrote
the dict finding functions to allow for multicore processing because it
would be brutally slow otherwise.

Files Added:
------------
assemble_data.py --> will assemble all the data together (numeric, categorical,
                 date), then split it into train/test/valid. Not complete,
                 waiting until I can impute the values for 'train_date.csv'.
find_dict_values_date.py --> finds the mean, mode, median of the date csv


----------------------------------------------------------------------------(80)
9 / 30 / 2016
==============
Had an issue running `assemble.py` with the data throwing me weird errors about
Memory. I had trouble following it as my RAM was gone by the time I got back so
I chose to implement loggers and try a different 'n' value. Those values are
stored in `assemble_1.log`. It didn't work either, also got weird errors that I
now forget (should've written down!).

I'll be running the new assemble.py file tomorrow back at n=100000 and seeing
what the logs put out. 

Maybe I should try building the `readin_data.py` file but instead have it
parse out the same Xth line to be used for validation later?


Files Added:
------------
assemble_1.log --> the results of the 'assemble.py' file being run at n=10000

----------------------------------------------------------------------------(80)
10 / 02 / 2016
==============
I tried running at n=100000 for `assembly_data.py`, but I had a MemoryError. I
didn't bother saving the log because it barely got past the first 100000. I
think it saved part of it to the log file.

I then tried n=50000 and got the log file that can be seen in `assemble_2.log`.
It didn't work out too well, I got a MemoryError towards the end.

I think that late idea I had to spread out the `readin_data.py` file will be the
best idea to make this effective. 


Files Added:
------------
assemble_2.log --> Results of n=50000

----------------------------------------------------------------------------(80)
10 / 06 / 2016
=============== 
Building out my `readin_data.py` to read in the data, but cut out every xth few
lines, which output to a validation and test dataset.

Predicting the 'response' variable remember.

I had some major issues stemming form the dataframe where the dates for the
data are not all covered. Thus I had to figure out an assumption to put. I just
chose 0; eventually I can go back and do all the brute force work I did for the
numeric and categorical data, but I really just want to train something already!

I ran into an issue where some of my column headers are being put into the
dataframe at times, don't know how that happened. Thinking I can just drop them.


Files Added/Edited:
-------------------
readin_data.py --> changed to accept splits and produce train,valid, test
rf_model.py --> my first sketch of a random forest model

----------------------------------------------------------------------------(80)
10 / 08 / 2016
=============== 
So I thought my issue with occassional strings is a result of writing to CSV with
the headers, but now I'm not sure as none of my scripts will write with the
headers nor do they write.

Did that and then rewrote the `readin_data.py` file to read in the data and then
recast everything into more correct numbers i.e. an object as float32 when
applicable. But I still get errors! I'm missing something I'm sure. Posted it
below for records:
```
Traceback (most recent call last):
  File "rf_model.py", line 13, in <module>
    rf_model.fit(train.drop('Response',axis=1),train['Response']);
  File "/home/vincent/workspace/kaggle_projects/bosch_production_line_quality/.venv/local/lib/pytho
n2.7/site-packages/sklearn/ensemble/forest.py", line 212, in fit                                  
    X = check_array(X, dtype=DTYPE, accept_sparse="csc")
  File "/home/vincent/workspace/kaggle_projects/bosch_production_line_quality/.venv/local/lib/pytho
n2.7/site-packages/sklearn/utils/validation.py", line 398, in check_array                         
    _assert_all_finite(array)
  File "/home/vincent/workspace/kaggle_projects/bosch_production_line_quality/.venv/local/lib/pytho
n2.7/site-packages/sklearn/utils/validation.py", line 54, in _assert_all_finite                   
    " or a value too large for %r." % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
```

Files Added/Edited:
-------------------
readin_data.py --> changed it to cut out any rows that have strings; aka drop
               rows where the string in column 'Id' was 'Id'
