URL: https://www.kaggle.com/c/bosch-production-line-performance
Due: November 11th, 2016
----------------------------------------------------------------------------(80)
9/7/2016
=========
First off, the datasets are ***very large***. In fact, they cannot fit into
memory so a generator will need to be built to properly process them.

Second, there are __many__ features to consider:
::train_categorical.columns >>> length=2141  # all numerical
::train_numeric.columns >>> length=970       # pd.get_dummies(...)
::train_date.columns >>> length=1157         # all numerical
Altogether --> 4268 features(!)

Some are 'nan' for long periods of time (i.e. categories only appear in sparse
areas). This happens to all three, categorical seems to stick out as
particularly sparse though.

Lastly, none of the cateogires have english names, they are all coded out.

Eyeballing 'train_date' suggests that its categorical, so I'll treat it as such


Questions to Ask:
  * Are there any scikit learn methods that will accept 'nan' values gracefully?

Steps to build:
  * Impute missing data on _numerical, consider 'nan' its own categorical for
    _categorical and _date. Output these into new CSVs 
  * Join all three CSVs together into one big CSV (gulp)
  * A train/valid/test splitter that will split up all the data (gulp), which
    should ideally be done on my local machine if it can handle it. Save these
    to separate csv files.
  * Complete PCA/LDA/QDA/SVD and general new features, save these to separate
    CSV files as well (maybe __with__ the original data too?)
  * Use feature selectors like looking for high variance features and using
    overfitted randomForest (multiple times to check) to see which features
    are most necesary.
  * Throw this all into xgboost for a final model
  * Double check on the validation set for hyperparameter selection
  * Use the test set to do final checks


----------------------------------------------------------------------------(80)
9 / 8 / 2016
============
train_numeric has 1,183,748 lines in it

It takes a fairly long time to do the imputing. This can be seen in 'find_dict_value.py',
which I'll funnel in piece by piece into the main python.

I added a generator that I scalped from a previous contest as well. 

----------------------------------------------------------------------------(80)
9 / 10 / 2016
============
Might it make more sense to insert the mean and median values at compute time as
opposed to building whole new CSV files?

I decided to do just that as I had troubles with the imputed values into their
own CSVs.

I'm thinking that all joins for the data should be done on 'readin_data.py'

Looking into breaking up the category columns in different columns; there's an
issue though as pd.get_dummies(...) and np.unique(...) do not return correct
values. Trying to figre out a way around.

I'm trying to run the following as I type, but its taking a while. Using a list
to brute force search for unique values:
>>> unique = [used.append(x) for x in df.values.tolist() if x not in used]
http://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python
Getting weird results, it ran for a long time so I gave up (maybe ~90 minutes)

----------------------------------------------------------------------------(80)
9 / 11 / 2016
==============
So I figured out that the pd.get_dummies(...) was missing a parameter to account
for 'NaN' values, namely pd.get_dummies(df,dummy_na=True).

Figured out how to write categorical data via multicore processing because its
going to take a long time (~30 hours) otherwise.


----------------------------------------------------------------------------(80)
9 / 12 / 2016
=============
Added a time elapsed bit to the 'ohe_categorical.py' and its taking forever to
run, so I'll queue it up to run tomorrow morning while I'm at work.


----------------------------------------------------------------------------(80)
9 / 24 / 2016
=============
So I did run it on the 13th and now I'm getting back to this contest after a
few weeks. The categorical data when one hot encoded has 8538 features.
  -> 'test_categorical.csv' also needs to be run this way. Likely will be done on
     Monday.
Now I'm going to stitch it all together with 'assemble_data.py' to allow for
better splitting. Currently sitting on ~120gb of CSV data (oof).

I'm still getting 'NaN' values for some date values (like in 'L3_S51_D4255').
Given there are too many values, I'll have to impute values instead. Rewrote
the dict finding functions to allow for multicore processing because it
would be brutally slow otherwise.

Files Added:
------------
assemble_data.py --> will assemble all the data together (numeric, categorical,
                 date), then split it into train/test/valid. Not complete,
                 waiting until I can impute the values for 'train_date.csv'.
find_dict_values_date.py --> finds the mean, mode, median of the date csv


----------------------------------------------------------------------------(80)
9 / 30 / 2016
==============
Had an issue running `assemble.py` with the data throwing me weird errors about
Memory. I had trouble following it as my RAM was gone by the time I got back so
I chose to implement loggers and try a different 'n' value. Those values are
stored in `assemble_1.log`. It didn't work either, also got weird errors that I
now forget (should've written down!).

I'll be running the new assemble.py file tomorrow back at n=100000 and seeing
what the logs put out. 

Maybe I should try building the `readin_data.py` file but instead have it
parse out the same Xth line to be used for validation later?


Files Added:
------------
assemble_1.log --> the results of the 'assemble.py' file being run at n=10000

----------------------------------------------------------------------------(80)
10 / 02 / 2016
==============
I tried running at n=100000 for `assembly_data.py`, but I had a MemoryError. I
didn't bother saving the log because it barely got past the first 100000. I
think it saved part of it to the log file.

I then tried n=50000 and got the log file that can be seen in `assemble_2.log`.
It didn't work out too well, I got a MemoryError towards the end.

I think that late idea I had to spread out the `readin_data.py` file will be the
best idea to make this effective. 


Files Added:
------------
assemble_2.log --> Results of n=50000
